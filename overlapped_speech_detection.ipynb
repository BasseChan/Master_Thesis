{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "610b7fda",
   "metadata": {},
   "source": [
    "# Przygotowanie zbiorów danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b3beb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prepare_datasets import prepare_dialogue_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56efc068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 12.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap time: 9.802594951296536 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prepare_dialogue_dataset(100, 'data/dialogues/low_overlap', min_speakers=2, max_speakers=5, min_fragments_per_speaker=1, max_fragments_per_speaker=3, min_speaker_time=8,\n",
    "                        overlap_prob=0.3, min_overlap=1, max_overlap=8, allow_repeat_speakers=False,\n",
    "                        min_space=0.5, max_space=1.5, sr=16000, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "070f462c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 12.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap time: 18.193862224735852 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prepare_dialogue_dataset(100, 'data/dialogues/mid_overlap', min_speakers=2, max_speakers=5, min_fragments_per_speaker=1, max_fragments_per_speaker=3, min_speaker_time=8,\n",
    "                        overlap_prob=0.6, min_overlap=1, max_overlap=8, allow_repeat_speakers=False,\n",
    "                        min_space=0.5, max_space=1.5, sr=16000, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8bd42ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 11.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap time: 29.04153679710101 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prepare_dialogue_dataset(100, 'data/dialogues/high_overlap', min_speakers=2, max_speakers=5, min_fragments_per_speaker=1, max_fragments_per_speaker=3, min_speaker_time=8,\n",
    "                        overlap_prob=0.9, min_overlap=1, max_overlap=8, allow_repeat_speakers=False,\n",
    "                        min_space=0.5, max_space=1.5, sr=16000, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54888999",
   "metadata": {},
   "source": [
    "# Testy narzędzi do overlapped speech detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef3440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "from pyannote.audio import Model\n",
    "from pyannote.audio.pipelines import OverlappedSpeechDetection\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "\n",
    "model1 = Pipeline.from_pretrained(\"pyannote/overlapped-speech-detection\", use_auth_token=API_KEY)\n",
    "model2 = OverlappedSpeechDetection(Model.from_pretrained(\"pyannote/segmentation\", use_auth_token=API_KEY))\n",
    "model3 = OverlappedSpeechDetection(Model.from_pretrained(\"pyannote/segmentation-3.0\", use_auth_token=API_KEY))\n",
    "models = [model1, model2, model3]\n",
    "model_names = [\"overlapped-speech-detection\", \"segmentation\", \"segmentation-3.0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d96af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db29b1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overlap_times(ref_segments, pred_segments, total_time):\n",
    "    events = []\n",
    "    for start, end in ref_segments:\n",
    "        events.append((start, 'start', 'ref'))\n",
    "        events.append((end, 'end', 'ref'))\n",
    "    for start, end in pred_segments:\n",
    "        events.append((start, 'start', 'pred'))\n",
    "        events.append((end, 'end', 'pred'))\n",
    "\n",
    "    events.sort(key=lambda x: (x[0], x[1] == 'end'))\n",
    "\n",
    "    prev_time = 0\n",
    "    in_ref = False\n",
    "    in_pred = False\n",
    "    TP = FP = FN = TN = 0.0\n",
    "\n",
    "    for time, kind, source in events:\n",
    "        duration = time - prev_time\n",
    "\n",
    "        if duration > 0:\n",
    "            if in_ref and in_pred:\n",
    "                TP += duration\n",
    "            elif in_ref and not in_pred:\n",
    "                FN += duration\n",
    "            elif not in_ref and in_pred:\n",
    "                FP += duration\n",
    "            else:\n",
    "                TN += duration\n",
    "\n",
    "        if source == 'ref':\n",
    "            in_ref = kind == 'start' if not in_ref else kind != 'end'\n",
    "        elif source == 'pred':\n",
    "            in_pred = kind == 'start' if not in_pred else kind != 'end'\n",
    "\n",
    "        prev_time = time\n",
    "\n",
    "    if prev_time < total_time:\n",
    "        duration = total_time - prev_time\n",
    "        if in_ref and in_pred:\n",
    "            TP += duration\n",
    "        elif in_ref and not in_pred:\n",
    "            FN += duration\n",
    "        elif not in_ref and in_pred:\n",
    "            FP += duration\n",
    "        else:\n",
    "            TN += duration\n",
    "\n",
    "    return {'TP': TP, 'FP': FP, 'FN': FN, 'TN': TN}\n",
    "\n",
    "\n",
    "def test_overlap_detection(model, datasets, n=100):\n",
    "    all_results = {}\n",
    "    for dataset in datasets:\n",
    "        overlap_df = pd.read_csv(f'data/dialogues/{dataset}/overlaps.csv')\n",
    "        results = []\n",
    "        for i in range(n):\n",
    "            wav = librosa.load(f'data/dialogues/{dataset}/{i}.wav', sr=16000)[0]\n",
    "            pred_overlaps = model({\"waveform\": torch.Tensor(wav).unsqueeze(0), \"sample_rate\": 16000})\n",
    "            pred_overlaps = [[overlap.start, overlap.end] for overlap in pred_overlaps.itersegments()]\n",
    "            overlaps = [(row['start'], row['end']) for _, row in overlap_df[overlap_df['sample_id'] == i].iterrows()]\n",
    "            scores = compute_overlap_times(overlaps, pred_overlaps, len(wav) / 16000)\n",
    "            prec = scores['TP'] / (scores['TP'] + scores['FP']) if (scores['TP'] + scores['FP']) > 0 else 1 if scores['FN'] == 0 else 0\n",
    "            rec = scores['TP'] / (scores['TP'] + scores['FN']) if (scores['TP'] + scores['FN']) > 0 else 1 if scores['FP'] == 0 else 0\n",
    "            f1 = 2 * (prec * rec) / (prec + rec) if (prec + rec) > 0 else 0\n",
    "            results.append([prec, rec, f1])\n",
    "        results = np.mean(results, axis=0)\n",
    "        all_results[dataset] = results\n",
    "    return all_results\n",
    "\n",
    "\n",
    "def test_params(args, param, values):\n",
    "    datasets = ['low_overlap', 'mid_overlap', 'high_overlap']\n",
    "    results = {model: {dataset: [] for dataset in datasets} for model in model_names}\n",
    "    for value in tqdm(values):\n",
    "        curr_args = args.copy()\n",
    "        curr_args[param] = value\n",
    "        for model, model_name in zip(models, model_names):\n",
    "            model.instantiate(curr_args)\n",
    "        \n",
    "            result = test_overlap_detection(model, datasets, n=100)\n",
    "            for dataset, metric in result.items():\n",
    "                results[model_name][dataset].append(metric)\n",
    "\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    metric_names = [\"precision\", \"recall\", \"f1_score\"]\n",
    "\n",
    "    for i, metric in enumerate(metric_names):\n",
    "        for j, dataset in enumerate(datasets):\n",
    "            axes[i, j].set_title(f\"{metric} for {dataset} dataset\")\n",
    "            axes[i, j].set_xlabel(param)\n",
    "            axes[i, j].set_ylabel(metric)\n",
    "            for model in model_names:\n",
    "                axes[i, j].plot(values, [metrics[i] for metrics in results[model][dataset]], label=model)\n",
    "            axes[i, j].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b1b92e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_args = {\n",
    "    \"onset\": 0.5,\n",
    "    \"offset\": 0.5,\n",
    "    \"min_duration_on\": 0,\n",
    "    \"min_duration_off\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb97054c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:23<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstandard_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43monset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[41], line 80\u001b[0m, in \u001b[0;36mtest_params\u001b[1;34m(args, param, values)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model, model_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(models, model_names):\n\u001b[0;32m     78\u001b[0m     model\u001b[38;5;241m.\u001b[39minstantiate(curr_args)\n\u001b[1;32m---> 80\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtest_overlap_detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dataset, metric \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     82\u001b[0m         results[model_name][dataset]\u001b[38;5;241m.\u001b[39mappend(metric)\n",
      "Cell \u001b[1;32mIn[41], line 58\u001b[0m, in \u001b[0;36mtest_overlap_detection\u001b[1;34m(model, datasets, n)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[0;32m     57\u001b[0m     wav \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/dialogues/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m'\u001b[39m, sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 58\u001b[0m     pred_overlaps \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwaveform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msample_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16000\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     pred_overlaps \u001b[38;5;241m=\u001b[39m [[overlap\u001b[38;5;241m.\u001b[39mstart, overlap\u001b[38;5;241m.\u001b[39mend] \u001b[38;5;28;01mfor\u001b[39;00m overlap \u001b[38;5;129;01min\u001b[39;00m pred_overlaps\u001b[38;5;241m.\u001b[39mitersegments()]\n\u001b[0;32m     60\u001b[0m     overlaps \u001b[38;5;241m=\u001b[39m [(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m overlap_df[overlap_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m i]\u001b[38;5;241m.\u001b[39miterrows()]\n",
      "File \u001b[1;32mc:\\Studia\\.conda\\Lib\\site-packages\\pyannote\\audio\\core\\pipeline.py:327\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, file, **kwargs)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessors\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    325\u001b[0m     file \u001b[38;5;241m=\u001b[39m ProtocolFile(file, lazy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessors)\n\u001b[1;32m--> 327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Studia\\.conda\\Lib\\site-packages\\pyannote\\audio\\pipelines\\overlapped_speech_detection.py:227\u001b[0m, in \u001b[0;36mOverlappedSpeechDetection.apply\u001b[1;34m(self, file, hook)\u001b[0m\n\u001b[0;32m    225\u001b[0m         file[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCACHED_SEGMENTATION] \u001b[38;5;241m=\u001b[39m segmentations\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 227\u001b[0m     segmentations: SlidingWindowFeature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_segmentation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msegmentation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m hook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegmentation\u001b[39m\u001b[38;5;124m\"\u001b[39m, segmentations)\n\u001b[0;32m    233\u001b[0m overlapped_speech \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_binarize(segmentations)\n",
      "File \u001b[1;32mc:\\Studia\\.conda\\Lib\\site-packages\\pyannote\\audio\\core\\inference.py:421\u001b[0m, in \u001b[0;36mInference.__call__\u001b[1;34m(self, file, hook)\u001b[0m\n\u001b[0;32m    418\u001b[0m waveform, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39maudio(file)\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msliding\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslide\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhook\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    423\u001b[0m outputs: Union[np\u001b[38;5;241m.\u001b[39mndarray, Tuple[np\u001b[38;5;241m.\u001b[39mndarray]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(waveform[\u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__first_sample\u001b[39m(outputs: np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n",
      "File \u001b[1;32mc:\\Studia\\.conda\\Lib\\site-packages\\pyannote\\audio\\core\\inference.py:313\u001b[0m, in \u001b[0;36mInference.slide\u001b[1;34m(self, waveform, sample_rate, hook)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, num_chunks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size):\n\u001b[0;32m    311\u001b[0m     batch: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m chunks[c : c \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[1;32m--> 313\u001b[0m     batch_outputs: Union[np\u001b[38;5;241m.\u001b[39mndarray, Tuple[np\u001b[38;5;241m.\u001b[39mndarray]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m     _ \u001b[38;5;241m=\u001b[39m map_with_specifications(\n\u001b[0;32m    316\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mspecifications, __append_batch, outputs, batch_outputs\n\u001b[0;32m    317\u001b[0m     )\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Studia\\.conda\\Lib\\site-packages\\pyannote\\audio\\core\\inference.py:215\u001b[0m, in \u001b[0;36mInference.infer\u001b[1;34m(self, chunks)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 215\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_oom_error(exception):\n",
      "File \u001b[1;32mc:\\Studia\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Studia\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Studia\\.conda\\Lib\\site-packages\\pyannote\\audio\\models\\segmentation\\PyanNet.py:226\u001b[0m, in \u001b[0;36mPyanNet.forward\u001b[1;34m(self, waveforms)\u001b[0m\n\u001b[0;32m    223\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msincnet(waveforms)\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39mlstm[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonolithic\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 226\u001b[0m     outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch feature frame -> batch frame feature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    230\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m rearrange(outputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch feature frame -> batch frame feature\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Studia\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Studia\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Studia\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1123\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1120\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1135\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1137\u001b[0m         batch_sizes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m   1145\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_params(standard_args, \"onset\", [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf4d849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa71b652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8e54d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d0f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(interval1: tuple[float, float], interval2: tuple[float, float]) -> float:\n",
    "    start1, end1 = interval1\n",
    "    start2, end2 = interval2\n",
    "    intersection = max(0, min(end1, end2) - max(start1, start2))\n",
    "    union = (end1 - start1) + (end2 - start2) - intersection\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def test_overlap_detection(model, datasets, n=100):\n",
    "    all_results = {}\n",
    "    for dataset in datasets:\n",
    "        overlap_df = pd.read_csv(f'data/dialogues/{dataset}/overlaps.csv')\n",
    "        results = []\n",
    "        for i in range(n):\n",
    "            wav = librosa.load(f'data/dialogues/{dataset}/{i}.wav', sr=16000)[0]\n",
    "            pred_overlaps = model({\"waveform\": torch.Tensor(wav).unsqueeze(0), \"sample_rate\": 16000})\n",
    "            pred_overlaps = [[overlap.start, overlap.end] for overlap in pred_overlaps.itersegments()]\n",
    "            overlaps = [(row['start'], row['end']) for _, row in overlap_df[overlap_df['sample_id'] == i]]\n",
    "            results.append([])\n",
    "\n",
    "\n",
    "\n",
    "            ref_50_iou = [0] * len(overlaps)\n",
    "            gen_50_iou = [0] * len(pred_overlaps)\n",
    "            ref_75_iou = [0] * len(overlaps)\n",
    "            gen_75_iou = [0] * len(pred_overlaps)\n",
    "            ref_90_iou = [0] * len(overlaps)\n",
    "            gen_90_iou = [0] * len(pred_overlaps)\n",
    "            overlaps = [(row['start'], row['end']) for _, row in overlap_df[overlap_df['sample_id'] == i]]\n",
    "            for j, overlap in enumerate(overlaps):\n",
    "                for k, gen_overlap in enumerate(pred_overlaps):\n",
    "                    iou = intersection_over_union(overlap, gen_overlap)\n",
    "                    if iou >= 0.5:\n",
    "                        ref_50_iou[j] = 1\n",
    "                        gen_50_iou[k] = 1\n",
    "                    if iou >= 0.75:\n",
    "                        ref_75_iou[j] = 1\n",
    "                        gen_75_iou[k] = 1\n",
    "                    if iou >= 0.9:\n",
    "                        ref_90_iou[j] = 1\n",
    "                        gen_90_iou[k] = 1\n",
    "            prec_50 = np.mean(gen_50_iou) if len(gen_50_iou) > 0 else 1\n",
    "            recall_50 = np.mean(ref_50_iou) if len(ref_50_iou) > 0 else 1\n",
    "            f1_50 = 2 * prec_50 * recall_50 / (prec_50 + recall_50) if prec_50 + recall_50 > 0 else 0\n",
    "            prec_75 = np.mean(gen_75_iou) if len(gen_75_iou) > 0 else 1\n",
    "            recall_75 = np.mean(ref_75_iou) if len(ref_75_iou) > 0 else 1\n",
    "            f1_75 = 2 * prec_75 * recall_75 / (prec_75 + recall_75) if prec_75 + recall_75 > 0 else 0\n",
    "            prec_90 = np.mean(gen_90_iou) if len(gen_90_iou) > 0 else 1\n",
    "            recall_90 = np.mean(ref_90_iou) if len(ref_90_iou) > 0 else 1\n",
    "            f1_90 = 2 * prec_90 * recall_90 / (prec_90 + recall_90) if prec_90 + recall_90 > 0 else 0\n",
    "            results.append([prec_50, prec_75, prec_90, recall_50, recall_75, recall_90, f1_50, f1_75, f1_90])\n",
    "        all_results[dataset] = np.mean(results, axis=0)\n",
    "    return all_results\n",
    "\n",
    "    with open('overlaps.json', 'r') as f:\n",
    "        all_overlaps = json.load(f)\n",
    "    results = []\n",
    "    for i, overlaps in tqdm(list(enumerate(all_overlaps))):\n",
    "        connected_wav = librosa.load(f'overlap_samples/{i}.wav', sr=SR)[0]\n",
    "        gen_overlaps = overlap_detection({\"waveform\": torch.Tensor(connected_wav).unsqueeze(0), \"sample_rate\": SR})\n",
    "        gen_overlaps = [[overlap.start, overlap.end] for overlap in gen_overlaps.itersegments()]\n",
    "        ref_50_iou = [0] * len(overlaps)\n",
    "        gen_50_iou = [0] * len(gen_overlaps)\n",
    "        ref_75_iou = [0] * len(overlaps)\n",
    "        gen_75_iou = [0] * len(gen_overlaps)\n",
    "        ref_90_iou = [0] * len(overlaps)\n",
    "        gen_90_iou = [0] * len(gen_overlaps)\n",
    "        for i, overlap in enumerate(overlaps):\n",
    "            for j, gen_overlap in enumerate(gen_overlaps):\n",
    "                iou = intersection_over_union(overlap, gen_overlap)\n",
    "                if iou >= 0.5:\n",
    "                    ref_50_iou[i] = 1\n",
    "                    gen_50_iou[j] = 1\n",
    "                if iou >= 0.75:\n",
    "                    ref_75_iou[i] = 1\n",
    "                    gen_75_iou[j] = 1\n",
    "                if iou >= 0.9:\n",
    "                    ref_90_iou[i] = 1\n",
    "                    gen_90_iou[j] = 1\n",
    "        prec_50 = np.mean(gen_50_iou) if len(gen_50_iou) > 0 else 1\n",
    "        recall_50 = np.mean(ref_50_iou) if len(ref_50_iou) > 0 else 1\n",
    "        f1_50 = 2 * prec_50 * recall_50 / (prec_50 + recall_50) if prec_50 + recall_50 > 0 else 0\n",
    "        prec_75 = np.mean(gen_75_iou) if len(gen_75_iou) > 0 else 1\n",
    "        recall_75 = np.mean(ref_75_iou) if len(ref_75_iou) > 0 else 1\n",
    "        f1_75 = 2 * prec_75 * recall_75 / (prec_75 + recall_75) if prec_75 + recall_75 > 0 else 0\n",
    "        prec_90 = np.mean(gen_90_iou) if len(gen_90_iou) > 0 else 1\n",
    "        recall_90 = np.mean(ref_90_iou) if len(ref_90_iou) > 0 else 1\n",
    "        f1_90 = 2 * prec_90 * recall_90 / (prec_90 + recall_90) if prec_90 + recall_90 > 0 else 0\n",
    "        results.append([prec_50, prec_75, prec_90, recall_50, recall_75, recall_90, f1_50, f1_75, f1_90])\n",
    "    return np.mean(results, axis=0)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d729b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_params(args, param, values):\n",
    "    results = []\n",
    "    for value in values:\n",
    "        curr_args = args.copy()\n",
    "        curr_args[param] = value\n",
    "        for model, model_name in zip(models, model_names):\n",
    "            model.instantiate(curr_args)\n",
    "            \n",
    "\n",
    "            result = model(curr_args)\n",
    "            results.append((model, param, value, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd84f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_param(args, param, values):\n",
    "    results = []\n",
    "    for value in values:\n",
    "        curr_args = args.copy()\n",
    "        curr_args[param] = value\n",
    "        overlap_detection.instantiate(curr_args)\n",
    "        prec_50, prec_75, prec_90, recall_50, recall_75, recall_90, f1_50, f1_75, f1_90 = test_overlap_detection()\n",
    "        results.append([prec_50, prec_75, prec_90, recall_50, recall_75, recall_90, f1_50, f1_75, f1_90])\n",
    "    results = pd.DataFrame(results, columns=['precission (IoU >= 50)', 'precission (IoU >= 75)', 'precission (IoU >= 90)',\n",
    "                                             'recall (IoU >= 50)', 'recall (IoU >= 75)', 'recall (IoU >= 90)',\n",
    "                                             'f1_score (IoU >= 50)', 'f1_score (IoU >= 75)', 'f1_score (IoU >= 90)'])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    metric_names = [\"precision\", \"recall\", \"f1_score\"]\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        for j in range(3):\n",
    "            ax.plot(values, results.iloc[:, i * 3 + j], label=results.columns[i * 3 + j])\n",
    "        ax.set_xlabel(\"Parameter\")\n",
    "        ax.set_ylabel(\"Metric Value\")\n",
    "        ax.set_title(metric_names[i])\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5e40bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba50c56c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "245d206d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50dd2cfef6a44fe9a4396c64be4d174d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/5.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Studia\\.conda\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\wikto\\.cache\\torch\\pyannote\\models--pyannote--segmentation-3.0. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3afa58e66ee4c86ba9443eefb26a3af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.yaml:   0%|          | 0.00/399 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# instantiate the model\n",
    "from pyannote.audio import Model\n",
    "model = Model.from_pretrained(\n",
    "  \"pyannote/segmentation-3.0\", \n",
    "  use_auth_token=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efe6b743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio.pipelines import OverlappedSpeechDetection\n",
    "pipeline = OverlappedSpeechDetection(segmentation=model)\n",
    "HYPER_PARAMETERS = {\n",
    "  # remove overlapped speech regions shorter than that many seconds.\n",
    "  \"min_duration_on\": 0.0,\n",
    "  # fill non-overlapped speech regions shorter than that many seconds.\n",
    "  \"min_duration_off\": 0.0\n",
    "}\n",
    "pipeline.instantiate(HYPER_PARAMETERS)\n",
    "osd = pipeline(\"data/dialogues/high_overlap/3.wav\")\n",
    "# `osd` is a pyannote.core.Annotation instance containing overlapped speech regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fbcac83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<pyannote.core.annotation.Annotation at 0x1515395af90>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fff727c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Studia\\.conda\\Lib\\site-packages\\pytorch_lightning\\utilities\\migration\\migration.py:208: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.1.3 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\wikto\\.cache\\torch\\pyannote\\models--pyannote--segmentation\\snapshots\\059e96f964841d40f1a5e755bb7223f76666bba4\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.7.1, yours is 2.5.1+cu124. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyannote.audio.pipelines.overlapped_speech_detection.OverlappedSpeechDetection at 0x15153857750>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "\n",
    "overlap_detection = Pipeline.from_pretrained(\"pyannote/overlapped-speech-detection\", use_auth_token=\"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\")\n",
    "overlap_detection.instantiate(HYPER_PARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac6239d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<pyannote.core.annotation.Annotation at 0x151538d8950>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlap_detection(\"data/dialogues/high_overlap/3.wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
